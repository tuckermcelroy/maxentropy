---
title: "Maximum Entropy: Claims Vignette"
author: "Tucker McElroy"
date: '2024-09-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Claims

We examine the "National Unemployment Insurance Weekly Claims" data from the
[Department of Labor](https://oui.doleta.gov/unemploy/claims.asp), referred to
as *claims* for short. The data covers a span from January 7, 1967 through 
April 22, 2023. We apply R functions that implement the methodology described
in *Analysis of Crisis Effects via Maximum Entropy Adjustment*.
  
## Load Code

Load up the libraries needed.

```{r,echo=FALSE}
library(seasonal,warn.conflicts=FALSE)
library(devtools,warn.conflicts=FALSE)
library(Rcpp,warn.conflicts=FALSE)
```

Load up code from Maximum Entropy repository.

```{r,echo=FALSE}
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/polymult.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/ARMAauto.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/hend.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/x11filter.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/maxent.reg.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/maxent.prep.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/maxent.lik.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/maxent.fit.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/maxent.ev.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/maxent.plot.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/hpsa.r")
source_url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/main/ubgenerator.r")
```

Load an additional function.

```{r}
get_start <- function(x,lag)
{
  period <- frequency(x)
  new <- time(x)[1] + lag/period
  return(c(floor(new),1+period*(new-floor(new))))
}
```

## Load Data

The data was downloaded on May 18, 2023. We load it from the repo, and plot.
 
```{r}
claims <- read.table(url("https://raw.githubusercontent.com/tuckermcelroy/maxentropy/refs/heads/main/Claims.dat"))
begin <- c(1967,1)
period <- 52
claims <- ts(claims,frequency=period,start=begin)
plot(claims)
```

Graph the logged data.

```{r}
plot(log(claims),ylab="Log Claims",xlab="Year")
```

We focus on a sub-window from 2019 onwards, in original scale.

```{r}
dataNew.ts <- ts(claims[2714:2938],frequency=period,start=c(2019,1))
plot(dataNew.ts,ylab="Claims",xlab="Year")
points(dataNew.ts)
```

Set up variables for analysis.

```{r}
x <- dataNew.ts
n <- length(x)
```

## Outlier Specification

We begin with a liberal specification of outliers, based upon exploratory
analysis. We call this the *full specification*. The variables *ao* and *ls*
provide the indices $t_0$ for the outlier start times.

```{r}
ao <- seq(65,82)
ls <- c(64,83)
r <- length(union(ao,ls))
```

Generate another plot, where the AO effects are in red, and the start dates of
the LS effects are in green.

```{r}
data.ao <- rep(NA,n)
data.ao[ao] <- x[ao]
data.ls <- rep(NA,n)
data.ls[ls] <- x[ls]
plot(dataNew.ts,ylab="Claims",xlab="Year")
points(ts(data.ao,frequency=period,start=c(2019,1)),col=2)
points(ts(data.ls,frequency=period,start=c(2019,1)),col=3)
```

## Specify and Fit Time Series Model

We specify a SARIMA model of period 52. Variables $p$ and $q$ are the 
autoregressive order and moving average order, and $d$ is the total number of
regular differences. Variables $ps$ and $qs$ are the orders of the seasonal
autoregressive and seasonal moving average polynomials, and $ds$ is the order of
seasonal aggregation.

```{r}
p <- 4
q <- 3
ps <- 1
qs <- 0
d <- 1
ds <- 0
```

For the maximum entropy code, we need to introduce the polynomial trend regressor,
which is given by $t^d$ for $t=1,2,...,n$. The function *maxent.fit* will fit
the specified SARIMA model to the data. This relies on the function *maxent.lik* 
to compute the log Gaussian likelihood, and *maxent.prep* to compute various 
matrices described in the paper.

```{r}
datareg <- ts(cbind(x,seq(1,n)^d),start=start(x),frequency=period)
fit.mle <- maxent.fit(datareg,ao,ls,p,q,ps,qs,d,ds)
par.mle <- fit.mle[[1]]
psi.mle <- fit.mle[[2]]$par
print(par.mle)
```

The first parameter is the innovation variance, followed by parameters
corresponding to the autoregressive, moving average, seasonal autoregressive,
and seasonal moving average polynomial coefficients, all written in minus 
convention. The final parameter is the regression coefficient for the trend
polynomial. Next, we compute and plot the residuals.

```{r}
ts.resid <- ts(c(rep(NA,r+d+ds*period),fit.mle[[3]]),
               frequency=period,start=start(x))
plot(ts.resid)
```

We can check the residuals for autocorrelation.

```{r}
acf(ts.resid[-seq(1,r+d+ds*period)],lag.max = 4*period,main="Residual")
```

## Checking Other Specifications

We can try a more conservative choice of outliers, where we dispense with the
second level shift and several of the additive outliers. We call this the 
*partial specification*.

```{r}
ao.new <- c(65,66,67,68,69,71,72)
ls.new <- 64
r.new <- length(union(ao.new,ls.new))
```

We have a new plot for this reduced batch of outliers.

```{r}
data.ao <- rep(NA,n)
data.ao[ao.new] <- x[ao.new]
data.ls <- rep(NA,n)
data.ls[ls.new] <- x[ls.new]
plot(dataNew.ts,ylab="Claims",xlab="Year")
points(ts(data.ao,frequency=period,start=c(2019,1)),col=2)
points(ts(data.ls,frequency=period,start=c(2019,1)),col=3)
```

We keep the time series model the same, and refit.

```{r}
fit.mle.new <- maxent.fit(datareg,ao.new,ls.new,p,q,ps,qs,d,ds)
par.mle.new <- fit.mle.new[[1]]
psi.mle.new <- fit.mle.new[[2]]$par
print(par.mle.new)
```

We can examine the new model's residuals.

```{r}
ts.resid.new <- ts(c(rep(NA,r.new+d+ds*period),fit.mle.new[[3]]),
               frequency=period,start=start(x))
plot(ts.resid.new)
```

We can check the residuals for autocorrelation.

```{r}
acf(ts.resid.new[-seq(1,r.new+d+ds*period)],lag.max = 4*period,main="Residual")
```

Next, we can compare the divergence (or, -2 times the log likelihood) of the two
fitted models. The first model has divergence `r fit.mle[[2]]$objective`, and
the second model has divergence `r fit.mle.new[[2]]$objective`. This is clearly
worse, perhaps due to the model's inability to capture the reversion of the time
series to a new level after the crisis onset and finish. The test statistic of
the paper involves evaluating the likelihood of the full specification model
at the parameter estimates of the partial specification model. To do this,
we use *maxent.prep* to compute the differenced data and the matrix *B* of the
paper, and evaluate the divergence of the full model at *psi.mle.new*.

```{r}
div.new <- fit.mle.new[[2]]$objective
prep.full <- maxent.prep(datareg,ao,ls,d,ds)
x.diff <- prep.full[[1]]
B.mat <- prep.full[[2]]
div.full <- maxent.lik(psi.mle.new,x.diff,period,p,q,ps,qs,B.mat,1)
```

The divergence for the full specification is lower, since the partial 
specification is a restriction. We take the difference of the divergences, and
compare to chi-square critical values with degrees of freedom given by the
difference `r r-r.new` of the number of outliers.

```{r}
print(div.new-div.full)
print(qchisq(df=(r-r.new),.95))
```

The result is non-significant, indicating no tangible improvement using the 
partial specification.

## Obtain Shrinkage Extreme-Value Adjustment

We consider adjusting for the extreme values while also forecasting and
backcasting 20 weeks. To do this, we must modify the time series object by
adding NAs for the forecast and backcast times, and also modify the *ao* and
*ls* variables.

```{r}
H <- 20
x.ext <- ts(c(rep(NA,H),x,rep(NA,H)),start=get_start(x,-H),frequency=period)
datareg <- ts(cbind(x.ext,seq(1,n+2*H)-H),start=start(x.ext),frequency=period)
ao_mod <- ao+H
ls_mod <- ls+H
```

First, we consider full shrinkage, or standard extreme-value adjustment. This is
obtained by setting $\alpha = 1$ in formula (13) of the paper. We use the function
*maxent.ev* to provide various outputs, including the projection of extremes onto 
regular values, the shrinkage of extremes, and the entropified data vector. 

```{r}
alpha <- 1
out <- maxent.ev(datareg,ao_mod,ls_mod,psi.mle,p,q,ps,qs,d,ds,alpha)
```

The variable *x.casted* contains the sample vector with forecasts and backcasts
appended and prepended, while *mse.casted* is the mean squared error matrix for
the casts.  

```{r}
x.casted <- ts(out[[5]],start=start(x.ext),frequency=period)
mse.casted <- out[[7]]
```

We also compute *x.entropy*, which is the sample vector corrected for missing
values and extreme values; *mse.entropy* is its mean squared error matrix.

```{r}
x.entropy <- ts(out[[6]],start=start(x.ext),frequency=period)
mse.entropy <- out[[8]]
```

Finally, we compute the test statistic $\widehat{\mathcal{E}}$ from the paper,
for whether the specified outlier effects are present: the value is
`r out[[9]]*10^{-3}` in thousands, with chi-square critical value
`r qchisq(df=r,.95)`. The maximum entropy adjustment with full shrinkage is 
plotted along with the shaded uncertainty, using our function *maxent.plot*. 

```{r}
maxent.plot(x.ext,x.entropy,mse.entropy,option=2,top.bound=6,bot.bound=-2)
```

Second, we consider partial shrinkage with $\alpha = .0001$. 

```{r}
alpha <- .0001
out <- maxent.ev(datareg,ao_mod,ls_mod,psi.mle,p,q,ps,qs,d,ds,alpha)
x.casted <- ts(out[[5]],start=start(x.ext),frequency=period)
mse.casted <- out[[7]]
x.entropy <- ts(out[[6]],start=start(x.ext),frequency=period)
mse.entropy <- out[[8]]
maxent.plot(x.ext,x.entropy,mse.entropy,option=2,top.bound=6,bot.bound=-2)
```

## Alternative Results from RegARIMA

We can also get an extreme-value adjustment from the RegARIMA approach, because
there are no missing values. However, we must refit the model because now there
are additional model parameters corresponding to the outlier effects. First,
we construct the full regression matrix (together with the trend polynomial)
using the function *maxent.reg*.

```{r}
X.mat <- maxent.reg(n,ao,ls)
datareg <- ts(cbind(cbind(x,seq(1,n)^d),X.mat),start=start(x),frequency=period)
```

Then we fit the time series; the time series model specification is the same.

```{r}
fit.mle.reg <- maxent.fit(datareg,NULL,NULL,p,q,ps,qs,d,ds)
par.mle.reg <- fit.mle.reg[[1]]
psi.mle.reg <- fit.mle.reg[[2]]$par
print(par.mle.reg)
```
Comparing the first 9 parameter values to those of *par.mle*, we see there are 
some differences, and the innovation variance `r par.mle.reg[1]` is smaller than
`r par.mle[1]` (due to the additional number of parameters in the former model).
The GLS estimates of the outlier regression parameters (we exclude the first
mean parameter, corresponding to the polynomial trend coefficient) are given by

```{r}
beta.reg <- psi.mle.reg[(length(psi.mle.reg)-dim(X.mat)[2]+1):length(psi.mle.reg)]
```

Finally, the extreme-value adjustment (according to Proposition 2 of the paper) 
is given by the following:

```{r}
x.ev.reg <- x - X.mat %*% beta.reg
```

We make a comparison of the extreme-value adjustments: the ME is in blue with
shaded bounds, and overlaid is the RegARIMA in green. There is little discrepancy,
and the adjustments would be identical except for differences in parameter
estimates; also, the RegARIMA adjustment is not computed for the forecast and
backcast leads.  

```{r}
maxent.plot(x.ext,x.entropy,mse.entropy,option=2,top.bound=6,bot.bound=-2)
lines(x.ev.reg,col=3)
```


## Get Seasonal Adjustment

For the application of seasonal adjustment, we first define a set of linear
filters that remove weekly seasonality. The calculations are based off of
material in Appendix B of [McElroy and Livsey](https://arxiv.org/pdf/2201.02148).
Here we define weekly filters for trend and seasonal adjustment, using the
function *ubgenerator*.

```{r}
week.period <- 365.25/7
half.len <- floor(week.period/2)
p.seas <- 1
trend.filter <- ubgenerator(week.period,NULL,1000)
trend.filter <- trend.filter/sum(trend.filter)
detrend.filter <- c(rep(0,half.len),1,rep(0,half.len)) - trend.filter
seas.filter <- 0
for(j in 1:p.seas)
{
  week.periodj <- j*week.period
  half.lenj <- floor(week.periodj/2)
  seas.filterj <- ubgenerator(week.periodj,half.lenj-1,1000)
  seas.filterj <- polymult(seas.filterj,c(1,0,-1))
  seas.filter <- c(seas.filter,rep(0,length(seas.filterj)-length(seas.filter)))
  seas.filter <- seas.filter + seas.filterj
}
seas.filter <- c(rep(0,length(seas.filter)-1),seas.filter)
seas.filter <- seas.filter + rev(seas.filter)
seas.filter <- c(rep(0,(length(seas.filter)-1)/2),1,
                 rep(0,(length(seas.filter)-1)/2)) - seas.filter/(2*p.seas+1)
ss.filter <- polymult(detrend.filter,seas.filter)
shift <- (length(ss.filter)-1)/2
sa.filter <- c(1,rep(0,shift)) - rev(ss.filter[1:(shift+1)])
sa.filter <- c(rev(sa.filter),sa.filter[-1])
```

We now have *trend.filter*, *seas.filter*, and *sa.filter* for extracting trend,
seasonal, and non-seasonal components. Next, we obtain the matrices 
corresponding to these two latter filters, following the paper.

```{r}
n_seas <- n + 2*shift
psiMat_seas <- matrix(c(rep(0,n-1),ss.filter),nrow=1)
psiMat_sa <- matrix(c(rep(0,n-1),sa.filter),nrow=1)
for(i in 2:n)
{
  psiMat_seas <- rbind(c(psiMat_seas[1,-1],0),psiMat_seas)
  psiMat_sa <- rbind(c(psiMat_sa[1,-1],0),psiMat_sa)
}
```

Our next step is to generate a sufficient number of forecasts and backcasts,
which is based on computing half the length of the filters. The following code
is similar to that used in the extreme-value adjustment above.

```{r}
H <- shift
x.ext <- ts(c(rep(NA,H),x,rep(NA,H)),start=get_start(x,-H),frequency=period)
datareg <- ts(cbind(x.ext,seq(1,n+2*H)-H),start=start(x.ext),frequency=period)
ao_mod <- ao+H
ls_mod <- ls+H
```

Having modified the outlier specifications appropriately, we now run
the maximum entropy code with full shrinkage.

```{r}
alpha <- 1
out <- maxent.ev(datareg,ao_mod,ls_mod,psi.mle,p,q,ps,qs,d,ds,alpha)
x.casted <- ts(out[[5]],start=start(x.ext),frequency=period)
mse.casted <- out[[7]]
x.entropy <- ts(out[[6]],start=start(x.ext),frequency=period)
mse.entropy <- out[[8]]
```

At this point we have *x.entropy*, the data corrected for extremes and casted 
values; taking the difference with *x.casted* isolates the extreme-value
adjustment, to which we apply the filter matrix *psiMat_sa*. However, to obtain
the seasonal adjustment we must add the outlier effects back.

```{r}
x.evadjust <- x.casted - x.entropy
x.sa_evfree <- psiMat_sa %*% x.entropy
x.sa <- x.sa_evfree + x.evadjust[(H+1):(n_seas-H)]
x.sa <- ts(x.sa,start=start(x),frequency=period)
```

This process is repeated for the seasonal factor, albeit there are no outlier
effects to add back to the seasonal component.

```{r}
x.seas <- psiMat_seas %*% x.entropy
x.seas <- ts(x.seas,start=start(x),frequency=period)
```

Finally, we compute the error covariance matrices for these projections.

```{r}
mse.sa <- (psiMat_sa - diag(n_seas)[(H+1):(n_seas-H),]) %*% mse.entropy %*%
  t(psiMat_sa - diag(n_seas)[(H+1):(n_seas-H),])
mse.seas <- (psiMat_seas - diag(n_seas)[(H+1):(n_seas-H),]) %*% mse.entropy %*%
  t(psiMat_seas - diag(n_seas)[(H+1):(n_seas-H),])
```

The results can now be visualized.

```{r}
maxent.plot(x.ext,x.sa,mse.sa,option=2,top.bound=6,bot.bound=0)
```

We can check whether there is any seasonality left by examining the 
seasonal adjustment without extremes.

```{r}
spec.ar(x.sa_evfree)
```

The spectrum shows no peaks at seasonal frequencies, and the autocorrelation plot
does not have large positive values at seasonal lags, although there is a large 
negative value at lag 52.

```{r}
acf(diff(x.sa_evfree),lag=160)
```

